import sys, os
from pydantic import BaseModel
import mlflow
from crewai.flow import Flow, listen, start, and_, router
import time
from src.mainflow.utils.input_validation import is_valid_input
from mlflow.genai.scorers import RetrievalRelevance, RelevanceToQuery
from mlflow.genai.judges import custom_prompt_judge
from mlflow.genai.scorers import scorer
from mlflow.genai import register_prompt
import pandas as pd
from src.mainflow.crews.input_crew.input_validation_crew import InputValidationCrew
from src.mainflow.crews.planner_crew.crew import PlanningCrew
from src.mainflow.crews.web_crew.crew_new import WebCrew
from src.mainflow.crews.paper_crew.paper_crew import PaperCrew
from src.mainflow.crews.calendar_crew.crew import CalendarCrew
from src.mainflow.crews.study_plan_crew.crew import FinalStudyPlanCrew

mlflow.set_experiment("EY Junior Accelerator") #imposta l'esperimento


class State(BaseModel):
    question : str = ""
    user_info : str = ""
    plan : str = ""
    resources : str = ""
    papers : str = ""
    study_plan : str = ""
    calendar : str = ""

@scorer
def evaluate_search_quality(inputs, outputs):
    """
    Evaluates the relevance of retrieved documents to the user's query.
    
    Args:
        inputs: Dictionary containing the original user query
        outputs: Dictionary containing the retrieved documents
    
    Returns:
        Numeric score representing relevance (0.0 to 10.0)
    """
    relevance_prompt = """
    Assess the relevance of the following documents to the user's query. 
    Consider how well the documents address the user's intent and provide useful information.

    You must choose one of the following categories:

    [[highly_relevant]]: The documents are directly related to the user's query, providing comprehensive and accurate information that fully addresses the user's needs.

    [[somewhat_relevant]]: The documents are generally related to the user's query but may lack depth or miss some aspects of the user's intent. They provide useful information but could be improved.

    [[not_relevant]]: The documents do not adequately address the user's query. They may be off-topic, too vague, or fail to provide useful information.

    User query: {{input_query}}
    Retrieved documents: {{output_documents}}
    """
    relevance_judge = custom_prompt_judge(
        name="search_relevance_evaluator",
        prompt_template=relevance_prompt,
        model="azure:/gpt-4.1",  # or your preferred model
        numeric_values={
            "highly_relevant": 10.0,
            "somewhat_relevant": 5.0,
            "not_relevant": 0.0,
        },
    )
    return relevance_judge(input_query=inputs, output_documents=outputs)

@scorer
def evaluate_study_plan_quality(inputs, outputs):
    """
    Evaluates the quality of a study plan generated by CrewAI agents
    
    Args:
        inputs: Dictionary containing student request and context
        outputs: Dictionary containing the generated study plan
    
    Returns:
        Numeric score representing plan quality (0.0 to 1.0)
    """
    study_planning_prompt = """
    Evaluate the quality and effectiveness of a study plan created by an AI agent. 
    Assess how well the plan addresses the student's learning objectives, timeline, and educational needs.

    You must choose one of the following categories:

    [[excellent_plan]]: The study plan is comprehensive, well-structured, realistic, and perfectly tailored to the student's needs. It includes:
    - Clear learning objectives and milestones
    - Appropriate time allocation and realistic scheduling
    - Varied study methods and resources
    - Assessment checkpoints and progress tracking
    - Consideration of student's current level and constraints

    [[good_plan]]: The study plan is solid and helpful but may lack some detail or optimization. It addresses most requirements but could be enhanced with:
    - Better time management or pacing
    - More diverse learning resources
    - Clearer milestone definitions
    - Minor adjustments for student's specific needs

    [[poor_plan]]: The study plan is inadequate, unrealistic, or doesn't address the student's requirements:
    - Vague or missing objectives
    - Poor time allocation or unrealistic expectations
    - Lack of structured approach
    - Ignores student's constraints or level
    Study plan: {{input_plan}}
    Generated study plan: {{output_plan}}
    """
    study_plan_judge = custom_prompt_judge(
        name="study_plan_evaluator",
        prompt_template=study_planning_prompt,
        model="azure:/gpt-4.1",  # or your preferred model
        numeric_values={
            "excellent_plan": 10.0,
            "good_plan": 7.5,
            "poor_plan": 2.5,
        },
    )
    return study_plan_judge(input_plan=inputs, output_plan=outputs)

class MonitoringConfig():

    #salva l'ora di inizio per calcolare il tempo di esecuzione
    def __init__(self):
        self.start_time = time.time()

    def monitoring_crew(self,state: State, crew_output, crew, crew_name):
        """
        Enhanced monitoring that works alongside CrewAI autolog.
        Autolog will handle detailed agent/task tracing, while this provides workflow-level metrics.
        """
        with mlflow.start_run(run_name=f"{crew_name} Monitoring", nested=True):              
                mlflow.log_param("crew", crew_name)
                mlflow.log_param("input_question", state.question)
                mlflow.log_param("sanitized_input", crew_output.raw)
                mlflow.log_param("num_agents", len(crew.agents))
                execution_time = time.time() - self.start_time
                mlflow.log_metric("execution_time_seconds", round(execution_time, 2))
                mlflow.log_param("output_type", type(crew_output).__name__)
                mlflow.log_metric("output_length", len(str(crew_output.raw)))
                mlflow.log_metric("tokens_total", float(crew_output.token_usage.total_tokens))
                mlflow.log_metric("tokens_prompt", float(len(state.question)))
                mlflow.log_metric("tokens_completion", float(crew_output.token_usage.completion_tokens))
                mlflow.log_param("token_usage_details", str(crew_output.token_usage))
                if crew_name == "PlanningCrew":
                    feedback = evaluate_study_plan_quality(state.user_info, crew_output.raw)
                    mlflow.log_metric("study_plan_quality_score", feedback.value)
                    mlflow.log_param("study_plan_quality_feedback", feedback.rationale)
                if crew_name == "WebCrew":
                    feedback = evaluate_search_quality(state.plan, crew_output.raw)
                    mlflow.log_metric("search_relevance_score", feedback.value)
                    mlflow.log_param("search_relevance_feedback", feedback.rationale)

            


class Flow(Flow[State]):

    @start()
    def insert_topic(self):
        print("="*20, " Welcome to the EY Junior Accelerator! ", "="*20)
        
        question = input("Describe your role, past experience, learning goals: ")
        
        if not is_valid_input(question):
            print("Invalid input detected. Please avoid using escape sequences or empty inputs.")
            return self.insert_topic()
        self.state.question = question

    @listen(insert_topic)
    def sanitize_input(self):
        print("Sanitizing input")
        validation_crew = InputValidationCrew()
        monitor = MonitoringConfig()
        crew_output = validation_crew.crew().kickoff(
            inputs={"question": self.state.question}
        )
        monitor.monitoring_crew(self.state, crew_output, validation_crew, "InputValidationCrew")
        
        self.state.user_info = crew_output.raw
        print(self.state.user_info)
        
    @router
    def routing(self):
        if "error" in self.state.user_info.lower():
           return "insert_topic"
        else:
            return "generate_plan"

    @listen("generate_plan")
    def generate_plan(self):
        print("Generating plan")
        planning_crew = PlanningCrew()
        monitor = MonitoringConfig()
        crew_output = planning_crew.crew().kickoff(

            inputs={"user_info": self.state.user_info}
        )
        monitor.monitoring_crew(self.state, crew_output, planning_crew, "PlanningCrew")
        self.state.plan = crew_output.raw

        print("Output:", crew_output.raw)
        

    @listen(generate_plan)
    def web_search(self):
        print("Searching the web for resources")
        web_crew = WebCrew()
        monitor = MonitoringConfig()
        crew_output = web_crew.crew().kickoff(

            inputs={"plan": self.state.plan}
        )
        monitor.monitoring_crew(self.state, crew_output, web_crew, "WebCrew")
        self.state.resources = crew_output.raw

    @listen(web_search)
    def paper_research(self):
        print("Searching for academic papers")
        paper_crew = PaperCrew()
        monitor = MonitoringConfig()
        crew_output = paper_crew.crew().kickoff(

            inputs={"plan": self.state.plan}
        )
        monitor.monitoring_crew(self.state, crew_output, paper_crew, "PaperCrew")

        print("Papers crew output:", crew_output.raw)
        self.state.papers = crew_output.raw

    @listen(paper_research)
    def define_calendar(self):
        print("Defining calendar")
        calendar_crew = CalendarCrew()
        crew_output = calendar_crew.crew().kickoff(

            inputs={
                "web_resources": self.state.resources,
                "papers": self.state.papers,
                "plan": self.state.plan
            }
        )

        monitor = MonitoringConfig()
        monitor.monitoring_crew(self.state, crew_output, calendar_crew, "CalendarCrew")

        print("Calendar defined based on the plan, resources, and papers.")
        self.state.calendar = crew_output.raw

    @listen(define_calendar)
    def create_study_plan(self):
        print("Creating study plan")
        study_plan_crew = FinalStudyPlanCrew()
        monitor = MonitoringConfig()
        crew_output = study_plan_crew.crew().kickoff(

            inputs={
                "resources": self.state.resources,
                "papers": self.state.papers,
                "plan": self.state.plan,
                "calendar": self.state.calendar
            }
        )
        monitor.monitoring_crew(self.state, crew_output, study_plan_crew, "FinalStudyPlanCrew")
        
        evaluation_ethics_data = pd.DataFrame([{"inputs": {"question": self.state.question},
                                                "outputs": crew_output.raw}])
        mlflow.genai.evaluate(data=evaluation_ethics_data,scorers=[RelevanceToQuery(model = "azure:/gpt-4.1")])
        print(crew_output.raw)
        self.state.study_plan = crew_output.raw


def kickoff():
    """
    Main workflow execution with enhanced MLflow tracking.
    CrewAI autolog will automatically capture detailed traces for all crew operations.
    """
    with mlflow.start_run(run_name="EYFlow_with_Autolog") as run:
        mlflow.crewai.autolog()
        mlflow.log_param("workflow_type", "EY_Junior_Accelerator")
        try:
            
            flow = Flow()
            start_time = time.time()
            result = flow.kickoff()
            total_time = time.time() - start_time
            mlflow.log_metric("total_workflow_time", total_time)
            mlflow.log_param("workflow_status", "completed")
            print(f"🎉 Workflow completed successfully in {total_time:.2f} seconds")
        
            return result
            
        except Exception as e:
            mlflow.log_param("workflow_status", "failed")
            mlflow.log_param("error_message", str(e))
            print(f"❌ Workflow failed: {e}")
            raise
    
def plot():
    flow = Flow()
    flow.plot()

if __name__ == "__main__":
    kickoff()
